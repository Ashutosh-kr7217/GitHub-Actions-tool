name: Bootstrap Complete Infrastructure

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to bootstrap (dev, staging, prod)'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      confirm:
        description: 'Type "CONFIRM" to proceed with bootstrap'
        required: true
        type: string

env:
  AWS_REGION: "ap-south-1"
  TF_VAR_environment: ${{ github.event.inputs.environment || 'dev' }}

jobs:
  validate-input:
    runs-on: ubuntu-latest
    steps:
      - name: Check confirmation
        if: ${{ github.event.inputs.confirm != 'CONFIRM' }}
        run: |
          echo "You must type CONFIRM to proceed with bootstrap"
          exit 1
  
  check-prerequisites:
    needs: validate-input
    runs-on: ubuntu-latest
    outputs:
      state_bucket_exists: ${{ steps.check_s3.outputs.exists }}
      lock_table_exists: ${{ steps.check_dynamodb.outputs.exists }}
      vpc_exists: ${{ steps.check_vpc.outputs.exists }}
    
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: true
          
      - name: Check S3 state bucket
        id: check_s3
        run: |
          BUCKET_NAME="tf-state-aws-infra01"
          if aws s3api head-bucket --bucket "$BUCKET_NAME" 2>/dev/null; then
            echo "S3 bucket $BUCKET_NAME exists"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "S3 bucket $BUCKET_NAME does not exist"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Check DynamoDB lock table
        id: check_dynamodb
        run: |
          TABLE_NAME="terraform-lock"
          if aws dynamodb describe-table --table-name "$TABLE_NAME" 2>/dev/null; then
            echo "DynamoDB table $TABLE_NAME exists"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "DynamoDB table $TABLE_NAME does not exist"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Check for existing VPC
        id: check_vpc
        run: |
          ENV="${{ github.event.inputs.environment }}"
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=secure-app-infra-$ENV-vpc" --query "Vpcs[0].VpcId" --output text)
          
          if [[ "$VPC_ID" != "None" && -n "$VPC_ID" ]]; then
            echo "VPC already exists: $VPC_ID"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "No existing VPC found for environment $ENV"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

  cleanup-pending-resources:
    needs: validate-input
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: true
          
      - name: Check and force-delete pending secret deletions
        run: |
          ENV="${{ github.event.inputs.environment }}"
          SECRET_ID="secure-app-infra/$ENV/app"
          
          echo "Checking if secret '$SECRET_ID' exists and is pending deletion..."
          
          # Try to describe the secret (will succeed if it exists, even if scheduled for deletion)
          if aws secretsmanager describe-secret --secret-id "$SECRET_ID" 2>/dev/null; then
            # Check if scheduled for deletion
            DELETE_DATE=$(aws secretsmanager describe-secret --secret-id "$SECRET_ID" --query "DeletedDate" --output text)
            
            if [[ "$DELETE_DATE" != "None" && -n "$DELETE_DATE" ]]; then
              echo "Secret '$SECRET_ID' is scheduled for deletion, forcing immediate deletion..."
              aws secretsmanager delete-secret --secret-id "$SECRET_ID" --force-delete-without-recovery
              echo "Secret force-deleted successfully. Waiting for deletion to complete..."
              sleep 5
            else
              echo "Secret exists but is not scheduled for deletion."
            fi
          else
            echo "Secret '$SECRET_ID' does not exist. No cleanup needed."
          fi

  setup-terraform-backend:
    needs: [check-prerequisites, cleanup-pending-resources]
    if: needs.check-prerequisites.outputs.state_bucket_exists == 'false' || needs.check-prerequisites.outputs.lock_table_exists == 'false'
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Create S3 bucket for Terraform state
        if: needs.check-prerequisites.outputs.state_bucket_exists == 'false'
        run: |
          BUCKET_NAME="tf-state-aws-infra01"
          
          # Create bucket
          aws s3 mb "s3://$BUCKET_NAME" --region ${{ env.AWS_REGION }}
            
          # Enable versioning
          aws s3api put-bucket-versioning \
            --bucket "$BUCKET_NAME" \
            --versioning-configuration Status=Enabled
              
          # Enable encryption
          aws s3api put-bucket-encryption \
            --bucket "$BUCKET_NAME" \
            --server-side-encryption-configuration '{
              "Rules": [
                {
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }
              ]
            }'
            
          # Add bucket policy to block public access
          aws s3api put-public-access-block \
            --bucket "$BUCKET_NAME" \
            --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
              
          echo "Created Terraform state bucket: $BUCKET_NAME"
          
      - name: Create DynamoDB table for state locking
        if: needs.check-prerequisites.outputs.lock_table_exists == 'false'
        run: |
          TABLE_NAME="terraform-lock"
          
          aws dynamodb create-table \
            --table-name "$TABLE_NAME" \
            --attribute-definitions AttributeName=LockID,AttributeType=S \
            --key-schema AttributeName=LockID,KeyType=HASH \
            --billing-mode PAY_PER_REQUEST
              
          echo "Created Terraform lock table: $TABLE_NAME"
          # Wait for table to be active before proceeding
          aws dynamodb wait table-exists --table-name "$TABLE_NAME"

  build-infrastructure:
    needs: [check-prerequisites, cleanup-pending-resources, setup-terraform-backend]
    if: always() && needs.check-prerequisites.outputs.vpc_exists == 'false'
    runs-on: ubuntu-latest
    outputs:
      vpc_id: ${{ steps.terraform_outputs.outputs.vpc_id }}
      public_subnet_ids: ${{ steps.terraform_outputs.outputs.public_subnet_ids }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.0
      
      - name: Verify Terraform files
        run: |
          if [ ! -d "terraform" ]; then
            echo "❌ Error: terraform directory not found!"
            exit 1
          fi
          
          if [ ! -f "terraform/main.tf" ]; then
            echo "❌ Error: terraform/main.tf not found!"
            exit 1
          fi
      
      - name: Initialize Terraform
        working-directory: terraform
        run: |
          terraform init
          
          # Select or create workspace for environment
          ENV="${{ github.event.inputs.environment }}"
          terraform workspace select $ENV || terraform workspace new $ENV
      
      - name: Deploy base infrastructure
        working-directory: terraform
        run: |
          # Plan and apply with default Amazon Linux AMI first
          terraform plan -out=tfplan
          terraform apply -auto-approve tfplan
      
      - name: Get Terraform outputs
        id: terraform_outputs
        working-directory: terraform
        run: |
          ../scripts/extract-terraform-outputs.sh .
          
  build-application-ami:
    needs: [check-prerequisites, build-infrastructure]
    # Run if VPC already exists OR if we just built it
    if: always() && (needs.check-prerequisites.outputs.vpc_exists == 'true' || needs.build-infrastructure.result == 'success')
    runs-on: ubuntu-latest
    outputs:
      ami_id: ${{ steps.ami.outputs.ami_id }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Setup Packer
        uses: hashicorp/setup-packer@main
        with:
          version: '1.9.4'

      - name: Install Packer plugins
        run: |
          echo "Installing required Packer plugins..."
          packer plugins install github.com/hashicorp/amazon
          
      - name: Set build timestamp
        id: build_timestamp
        run: |
          BUILD_TIMESTAMP=$(date +%Y%m%d%H%M%S)
          echo "BUILD_TIMESTAMP=$BUILD_TIMESTAMP" >> $GITHUB_OUTPUT
          
      - name: Create deployment artifacts
        run: |
          echo "Creating deployment package..."
          mkdir -p deploy
          
          # Check if app directory exists and has content
          if [ -d "app" ] && [ "$(ls -A app 2>/dev/null)" ]; then
            echo "Copying app files to deploy directory"
            cp -r app/* deploy/
          else
            echo "App directory is empty or missing. Creating sample content..."
            mkdir -p deploy
            cat > deploy/index.html << 'EOFHTML'
          <!DOCTYPE html>
          <html>
          <head>
            <title>Secure Application</title>
            <style>
              body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
              h1 { color: #333; }
            </style>
          </head>
          <body>
            <h1>Secure Application Infrastructure</h1>
            <p>Build timestamp: ${BUILD_TIMESTAMP}</p>
            <p>App version: ${APP_VERSION}</p>
            <p>Deployment completed on $(date)</p>
          </body>
          </html>
          EOFHTML
          
            cat > deploy/style.css << 'EOFCSS'
          body {
            font-family: Arial, sans-serif;
            margin: 40px;
            line-height: 1.6;
            color: #333;
          }
          h1 {
            color: #0066cc;
            border-bottom: 1px solid #eee;
            padding-bottom: 10px;
          }
          EOFCSS
          fi
          
          # Show what we've created
          echo "Files in deploy directory:"
          ls -la deploy/
      
      - name: Check for Packer directory
        id: check_packer
        run: |
          if [ ! -d "packer" ]; then
            echo "❌ Error: packer directory not found!"
            mkdir -p packer
            echo "Creating minimal packer directory and config file"
            
            # Create a basic packer config
            cat > packer/application.json << 'EOF'
          {
            "variables": {
              "aws_region": "ap-south-1",
              "app_version": "{{env `APP_VERSION`}}",
              "build_timestamp": "{{env `BUILD_TIMESTAMP`}}",
              "subnet_id": "{{env `SUBNET_ID`}}"
            },
            "builders": [
              {
                "type": "amazon-ebs",
                "region": "{{user `aws_region`}}",
                "subnet_id": "{{user `subnet_id`}}",
                "associate_public_ip_address": true,
                "source_ami_filter": {
                  "filters": {
                    "virtualization-type": "hvm",
                    "name": "amzn2-ami-hvm-*-x86_64-gp2",
                    "root-device-type": "ebs"
                  },
                  "owners": ["amazon"],
                  "most_recent": true
                },
                "instance_type": "t3.small",
                "ssh_username": "ec2-user",
                "ami_name": "secure-app-infra-{{user `build_timestamp`}}",
                "tags": {
                  "Name": "secure-app-infra-ami",
                  "Environment": "dev",
                  "Project": "secure-app-infra",
                  "BuildTimestamp": "{{user `build_timestamp`}}",
                  "AppVersion": "{{user `app_version`}}",
                  "ManagedBy": "Packer"
                }
              }
            ],
            "provisioners": [
              {
                "type": "shell",
                "inline": [
                  "echo 'Installing dependencies...'",
                  "sudo yum update -y",
                  "sudo amazon-linux-extras enable nginx1",
                  "sudo yum install -y nginx git"
                ]
              },
              {
                "type": "shell",
                "inline": [
                  "mkdir -p ~/website"
                ]
              },
              {
                "type": "file",
                "source": "../deploy/",
                "destination": "~/website/"
              },
              {
                "type": "shell",
                "inline": [
                  "sudo mkdir -p /usr/share/nginx/html",
                  "sudo cp -r ~/website/* /usr/share/nginx/html/",
                  "sudo chown -R nginx:nginx /usr/share/nginx/html",
                  "sudo systemctl enable nginx",
                  "sudo systemctl start nginx"
                ]
              }
            ]
          }
          EOF
          else
            echo "✅ Packer directory exists"
          fi
          
      - name: Get subnet for Packer
        id: subnet
        run: |
          # Try to use subnet from infrastructure job if available
          if [[ -n "${{ needs.build-infrastructure.outputs.public_subnet_ids }}" && "${{ needs.build-infrastructure.outputs.public_subnet_ids }}" != "" ]]; then
            # Extract first subnet from the comma-separated list (not JSON)
            SUBNET_ID=$(echo "${{ needs.build-infrastructure.outputs.public_subnet_ids }}" | cut -d ',' -f1)
            echo "Using subnet from terraform output: $SUBNET_ID"
          else
            # If not available, find a public subnet from existing infrastructure
            echo "Finding available public subnet..."
            ENV="${{ github.event.inputs.environment }}"
            SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=tag:Name,Values=secure-app-infra-$ENV-public-*" "Name=state,Values=available" --query "Subnets[0].SubnetId" --output text)
      
            # If still not found, get any available subnet
            if [[ "$SUBNET_ID" == "None" || -z "$SUBNET_ID" ]]; then
              echo "No project subnet found, searching for any available subnet..."
              SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=state,Values=available" --query "Subnets[0].SubnetId" --output text)
            fi
          fi
    
          if [[ "$SUBNET_ID" == "None" || -z "$SUBNET_ID" ]]; then
            echo "No subnets found. Cannot proceed with Packer build."
            exit 1
          fi
    
          echo "SUBNET_ID=$SUBNET_ID" >> $GITHUB_OUTPUT
          echo "Found subnet ID: $SUBNET_ID"
          
      - name: Run Packer validation
        working-directory: packer
        run: |
          # First check if application.json exists
          if [ ! -f "application.json" ]; then
            echo "❌ Error: application.json not found in packer directory!"
            exit 1
          fi
        
          packer validate \
            -var "build_timestamp=${{ steps.build_timestamp.outputs.BUILD_TIMESTAMP }}" \
            -var "app_version=v1.0.${{ github.run_number }}" \
            -var "subnet_id=${{ steps.subnet.outputs.SUBNET_ID }}" \
            application.json
          
      - name: Build AMI with Packer
        working-directory: packer
        env:
          BUILD_TIMESTAMP: ${{ steps.build_timestamp.outputs.BUILD_TIMESTAMP }}
          APP_VERSION: v1.0.${{ github.run_number }}
          SUBNET_ID: ${{ steps.subnet.outputs.SUBNET_ID }}
        run: |
          packer build \
            -var "build_timestamp=${BUILD_TIMESTAMP}" \
            -var "app_version=${APP_VERSION}" \
            -var "subnet_id=${SUBNET_ID}" \
            application.json
          
      - name: Get AMI ID
        id: ami
        run: |
          sleep 10
          # Write to file first, then read - avoid command substitution issues
          aws ec2 describe-images \
            --owners self \
            --filters "Name=name,Values=secure-app-infra-${{ steps.build_timestamp.outputs.BUILD_TIMESTAMP }}" \
            --query "Images[0].ImageId" \
            --output text > ami_id.txt
          
          # Check if AMI ID was found
          if [ ! -s ami_id.txt ] || [ "$(cat ami_id.txt)" == "None" ]; then
            echo "Failed to find AMI. Check Packer build logs."
            exit 1
          fi
          
          # Set output
          echo "ami_id=$(cat ami_id.txt)" >> $GITHUB_OUTPUT
          echo "Built AMI ID: $(cat ami_id.txt)"
          
          # Clean up
          rm -f ami_id.txt
          
  deploy-with-custom-ami:
    needs: [build-infrastructure, build-application-ami]
    if: always() && needs.build-application-ami.result == 'success'
    runs-on: ubuntu-latest
    outputs:
      alb_dns: ${{ steps.terraform_outputs.outputs.alb_dns_name }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.0
      
      - name: Initialize Terraform
        working-directory: terraform
        run: |
          terraform init
          
          # Select workspace for environment
          ENV="${{ github.event.inputs.environment }}"
          terraform workspace select $ENV || terraform workspace new $ENV
      
      - name: Update Launch Template AMI directly with AWS CLI
        run: |
          echo "Updating Launch Template with AMI ID: ${{ needs.build-application-ami.outputs.ami_id }}"
          
          # Find the launch template by name
          LAUNCH_TEMPLATE_NAME="secure-app-infra-${{ github.event.inputs.environment || 'dev' }}-launch-template"
          
          # Check if the launch template exists
          if LAUNCH_TEMPLATE_ID=$(aws ec2 describe-launch-templates \
              --filters "Name=launch-template-name,Values=$LAUNCH_TEMPLATE_NAME" \
              --query "LaunchTemplates[0].LaunchTemplateId" --output text); then
              
            if [[ "$LAUNCH_TEMPLATE_ID" != "None" && -n "$LAUNCH_TEMPLATE_ID" ]]; then
              echo "Found Launch Template: $LAUNCH_TEMPLATE_ID"
              
              # Create a new version with the new AMI
              NEW_VERSION=$(aws ec2 create-launch-template-version \
                --launch-template-id "$LAUNCH_TEMPLATE_ID" \
                --source-version '$Latest' \
                --version-description "Updated AMI to ${{ needs.build-application-ami.outputs.ami_id }}" \
                --launch-template-data "{\"ImageId\": \"${{ needs.build-application-ami.outputs.ami_id }}\"}" \
                --query "LaunchTemplateVersion.VersionNumber" \
                --output text)
                
              echo "Created new launch template version: $NEW_VERSION"
              
              # Set as default version
              aws ec2 modify-launch-template \
                --launch-template-id "$LAUNCH_TEMPLATE_ID" \
                --default-version "$NEW_VERSION"
                
              echo "✅ Successfully updated launch template with AMI ID: ${{ needs.build-application-ami.outputs.ami_id }}"
              
              # Start an instance refresh if ASG exists
              ASG_NAME="secure-app-infra-${{ github.event.inputs.environment || 'dev' }}-asg"
              if aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names "$ASG_NAME" 2>/dev/null; then
                echo "Starting instance refresh for ASG: $ASG_NAME"
                aws autoscaling start-instance-refresh \
                  --auto-scaling-group-name "$ASG_NAME" \
                  --preferences '{"MinHealthyPercentage": 90, "InstanceWarmup": 300}'
              fi
            else
              echo "Launch template not found - proceeding with normal Terraform apply"
              cd terraform
              terraform apply -auto-approve -var="app_ami_id=${{ needs.build-application-ami.outputs.ami_id }}"
            fi
          else
            echo "Error querying launch template - proceeding with normal Terraform apply"
            cd terraform
            terraform apply -auto-approve -var="app_ami_id=${{ needs.build-application-ami.outputs.ami_id }}"
          fi
      
      - name: Get Terraform outputs
        id: terraform_outputs
        working-directory: terraform
        run: |
          ../scripts/extract-terraform-outputs.sh .

  verify-deployment:
    needs: deploy-with-custom-ami
    if: |
      always() && 
      needs.deploy-with-custom-ami.result != 'skipped'
    runs-on: ubuntu-latest
    steps:
      # Add debug step
      - name: Debug outputs
        run: |
          echo "All outputs from deploy-with-custom-ami job:"
          echo "alb_dns_name: ${{ needs.deploy-with-custom-ami.outputs.alb_dns_name }}"
          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
    
      - name: Verify deployment
        run: |
          ALB_DNS="${{ needs.deploy-with-custom-ami.outputs.alb_dns_name }}"
          
          if [[ -z "$ALB_DNS" ]]; then
            echo "⚠️ No ALB DNS provided, skipping verification"
            echo "Setting bootstrap status anyway as the deployment completed."
          else
            echo "Verifying application at http://${ALB_DNS}..."
            
            # Wait for ALB to be available with timeout
            MAX_RETRIES=30
            RETRY=0
            
            while [ $RETRY -lt $MAX_RETRIES ]; do
              HTTP_STATUS=$(curl -s -o response.html -w "%{http_code}" -m 10 "http://${ALB_DNS}/" || echo "000")
              
              if [[ "$HTTP_STATUS" == "200" ]]; then
                echo "✅ Application is accessible! HTTP status: $HTTP_STATUS"
                echo "Deployment completed successfully by ${{ github.actor }} at $(date -u '+%Y-%m-%d %H:%M:%S')"
                break
              else
                RETRY=$((RETRY+1))
                echo "Waiting for application to become available (attempt $RETRY/$MAX_RETRIES)..."
                sleep 10
                
                if [ $RETRY -eq $MAX_RETRIES ]; then
                  echo "⚠️ Timeout waiting for application, but continuing with bootstrap status..."
                fi
              fi
            done
          fi

      - name: Set bootstrap status
        run: |
          # Create status file
          echo "{\"status\":\"bootstrapped\",\"timestamp\":\"$(date -u '+%Y-%m-%d %H:%M:%S')\",\"actor\":\"${{ github.actor }}\"}" > status.json
          
          # Upload to S3
          aws s3 cp status.json s3://tf-state-aws-infra01/secure-app-infra/workflow-status.json
          
          echo "Bootstrap status recorded successfully"
