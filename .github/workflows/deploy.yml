name: Infrastructure and Application Deployment

#on:
  #push:
    #branches: [ main ]
    #paths-ignore:
      - 'app/**'  # Ignore app changes since they trigger the build-ami workflow
      - '**.md'   # Ignore markdown files
      - '.github/ISSUE_TEMPLATE/**'
      - '.github/PULL_REQUEST_TEMPLATE.md'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      rebuild_ami:
        description: 'Rebuild AMI before deploying'
        type: boolean
        default: false
      debug_mode:
        description: 'Enable verbose logging'
        type: boolean
        default: false
      dry_run:
        description: 'Plan only - do not apply changes'
        type: boolean
        default: false

concurrency: 
  group: ${{ github.workflow }}-${{ inputs.environment || 'dev' }}
  cancel-in-progress: false

env:
  TF_VAR_environment: ${{ inputs.environment || 'dev' }}
  AWS_REGION: ap-south-1
  DEBUG_MODE: ${{ inputs.debug_mode || false }}

jobs:
  check_prerequisites:
    runs-on: ubuntu-latest
    outputs:
      proceed: ${{ steps.check.outputs.proceed }}
      ami_rebuild_required: ${{ steps.check_ami.outputs.rebuild_required }}
      ami_id: ${{ steps.get_ami.outputs.ami_id }}
      
    steps:
      - name: Check for AMI build in progress
        id: check
        run: |
          # Default to proceeding in case we cannot check
          echo "proceed=true" >> $GITHUB_OUTPUT
          
          # Check if we have permissions to check workflow runs
          if [[ -n "${{ secrets.GITHUB_TOKEN }}" ]]; then
            RUNNING_AMI_BUILDS=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
              "https://api.github.com/repos/${{ github.repository }}/actions/runs?status=in_progress&event=push" | \
              jq '.workflow_runs[] | select(.name=="Build Application AMI") | .id' | wc -l)
            
            if [ "$RUNNING_AMI_BUILDS" -gt 0 ]; then
              echo "AMI build workflow is running. Waiting..."
              echo "proceed=false" >> $GITHUB_OUTPUT
            else
              echo "No AMI build in progress. Proceeding with deployment."
              echo "proceed=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "GITHUB_TOKEN not available with sufficient permissions, skipping AMI build check"
          fi
          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Check bootstrap status
        run: |
          # Check if infrastructure is bootstrapped
          if ! aws s3 ls s3://tf-state-aws-infra011/secure-app-infra/workflow-status.json 2>/dev/null; then
            echo "⚠️ Infrastructure bootstrap status not found!"
            echo "Run bootstrap-infrastructure.yml workflow first"
            exit 1
          fi
          
          # Download status file to confirm bootstrap status
          aws s3 cp s3://tf-state-aws-infra011/secure-app-infra/workflow-status.json ./status.json
          STATUS=$(cat status.json | jq -r '.status')
          
          if [[ "$STATUS" != "bootstrapped" ]]; then
            echo "⚠️ Infrastructure is not in bootstrapped state (current: $STATUS)"
            echo "Run bootstrap-infrastructure.yml workflow first"
            exit 1
          fi
          
          echo "✅ Bootstrap status verified"
          
      - name: Determine if AMI rebuild is needed
        id: check_ami
        run: |
          # Check if user manually requested AMI rebuild
          if [[ "${{ inputs.rebuild_ami }}" == "true" ]]; then
            echo "AMI rebuild explicitly requested through workflow input"
            echo "rebuild_required=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Otherwise determine based on file changes and environment
          ENV="${{ inputs.environment || 'dev' }}"
          
          echo "No AMI rebuild required for this deployment"
          echo "rebuild_required=false" >> $GITHUB_OUTPUT

      - name: Get latest AMI ID
        id: get_ami
        run: |
          # Find the latest AMI for this project
          ENV="${{ inputs.environment || 'dev' }}"
          LATEST_AMI=$(aws ec2 describe-images \
            --owners self \
            --filters "Name=tag:Project,Values=secure-app-infra" "Name=tag:Environment,Values=$ENV" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --output text)
          
          if [[ "$LATEST_AMI" == "None" || -z "$LATEST_AMI" ]]; then
            echo "⚠️ No AMI found for environment: $ENV"
            
            # Fall back to any AMI from the project
            LATEST_AMI=$(aws ec2 describe-images \
              --owners self \
              --filters "Name=tag:Project,Values=secure-app-infra" \
              --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
              --output text)
              
            if [[ "$LATEST_AMI" == "None" || -z "$LATEST_AMI" ]]; then
              echo "❌ No AMI found for this project at all!"
              echo "ami_id=" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          echo "Found AMI ID: $LATEST_AMI"
          echo "ami_id=$LATEST_AMI" >> $GITHUB_OUTPUT
  
  trigger_ami_build:
    needs: check_prerequisites
    if: needs.check_prerequisites.outputs.ami_rebuild_required == 'true' && needs.check_prerequisites.outputs.proceed == 'true'
    runs-on: ubuntu-latest
    outputs:
      ami_id: ${{ steps.get_ami_id.outputs.ami_id }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Trigger AMI build workflow
        id: trigger
        run: |
          echo "Triggering AMI build workflow..."
          
          WORKFLOW_ID=$(curl -s \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            -H "Accept: application/vnd.github.v3+json" \
            -X POST \
            -d '{"ref":"${{ github.ref }}","inputs":{"debug_mode":${{ inputs.debug_mode || false }}}}' \
            "https://api.github.com/repos/${{ github.repository }}/actions/workflows/build-ami.yml/dispatches")
          
          echo "AMI build workflow triggered"
          echo "Waiting for AMI build to complete..."
          
      - name: Wait for AMI build to complete
        run: |
          MAX_WAIT=1800  # 30 minutes
          INTERVAL=30    # Check every 30 seconds
          ELAPSED=0
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            # Query for recent completed build-ami workflow runs
            RECENT_RUN=$(curl -s \
              -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
              "https://api.github.com/repos/${{ github.repository }}/actions/runs?workflow=build-ami.yml&status=completed&per_page=1" | \
              jq '.workflow_runs[0]')
              
            if [[ -n "$RECENT_RUN" && "$RECENT_RUN" != "null" ]]; then
              RUN_CREATED=$(echo $RECENT_RUN | jq -r '.created_at')
              RUN_ID=$(echo $RECENT_RUN | jq -r '.id')
              
              # Check if this run was created after our trigger
              TRIGGER_TIME=$(date -u -d "5 minutes ago" +"%Y-%m-%dT%H:%M:%SZ")
              
              if [[ "$RUN_CREATED" > "$TRIGGER_TIME" ]]; then
                CONCLUSION=$(echo $RECENT_RUN | jq -r '.conclusion')
                
                if [[ "$CONCLUSION" == "success" ]]; then
                  echo "✅ AMI build completed successfully (Run ID: $RUN_ID)"
                  break
                elif [[ "$CONCLUSION" != "null" && "$CONCLUSION" != "in_progress" ]]; then
                  echo "❌ AMI build failed with status: $CONCLUSION"
                  echo "Check the build-ami workflow run for details"
                  exit 1
                fi
              fi
            fi
            
            # Update progress
            ELAPSED=$((ELAPSED + INTERVAL))
            MINUTES=$((ELAPSED / 60))
            echo "Still waiting for AMI build... ($MINUTES minutes elapsed)"
            sleep $INTERVAL
            
            if [ $ELAPSED -ge $MAX_WAIT ]; then
              echo "⚠️ Timed out waiting for AMI build to complete"
              echo "Will proceed with deployment using existing AMI"
              break
            fi
          done
          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Get latest AMI ID
        id: get_ami_id
        run: |
          # Find the latest AMI for this project
          ENV="${{ inputs.environment || 'dev' }}"
          LATEST_AMI=$(aws ec2 describe-images \
            --owners self \
            --filters "Name=tag:Project,Values=secure-app-infra" "Name=tag:Environment,Values=$ENV" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --output text)
          
          if [[ "$LATEST_AMI" == "None" || -z "$LATEST_AMI" ]]; then
            # Fall back to any AMI from the project
            LATEST_AMI=$(aws ec2 describe-images \
              --owners self \
              --filters "Name=tag:Project,Values=secure-app-infra" \
              --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
              --output text)
          fi
          
          echo "Found AMI ID: $LATEST_AMI"
          echo "ami_id=$LATEST_AMI" >> $GITHUB_OUTPUT

  validate:
    needs: check_prerequisites
    if: needs.check_prerequisites.outputs.proceed == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Install validation tools
        run: |
          npm install -g html-validate@7.18.0 stylelint@15.10.3 stylelint-config-standard@34.0.0
          
      - name: Validate HTML
        if: hashFiles('app/*.html') != ''
        run: |
          echo '{"extends": ["html-validate:recommended"]}' > .htmlvalidate.json
          html-validate app/*.html || echo "HTML validation warnings found but continuing"
          
      - name: Validate CSS
        if: hashFiles('app/*.css') != ''
        run: |
          echo '{"extends": "stylelint-config-standard"}' > .stylelintrc.json
          npx stylelint "app/*.css" || echo "CSS validation warnings found but continuing"
      
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.0
      
      - name: Fix Terraform formatting
        id: fmt-fix
        run: terraform fmt -recursive
        working-directory: terraform
        continue-on-error: true
        
      - name: Terraform Format Check
        run: terraform fmt -check -recursive
        working-directory: terraform
      
      - name: Terraform Validate
        run: |
          terraform init -backend=false
          terraform validate
        working-directory: terraform
        
      - name: Print deployment summary
        run: |
          echo "==== Deployment Summary ===="
          echo "Environment: ${{ inputs.environment || 'dev' }}"
          echo "AMI rebuild: ${{ needs.check_prerequisites.outputs.ami_rebuild_required }}"
          echo "AMI ID: ${{ needs.check_prerequisites.outputs.ami_id || 'Will be determined during deployment' }}"
          echo "Dry run: ${{ inputs.dry_run || false }}"
          echo "Debug mode: ${{ env.DEBUG_MODE }}"
          echo "========================="
        
  terraform-infra:
    needs: [check_prerequisites, validate, trigger_ami_build]
    if: |
      always() && 
      needs.check_prerequisites.outputs.proceed == 'true' &&
      needs.validate.result == 'success' &&
      (needs.trigger_ami_build.result == 'success' || needs.trigger_ami_build.result == 'skipped')
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment || 'dev' }}
    outputs:
      bastion_ip: ${{ steps.terraform_outputs.outputs.bastion_ip }}
      alb_dns: ${{ steps.terraform_outputs.outputs.alb_dns }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Debug Workflow Context
        run: |
          echo "Event name: ${{ github.event_name }}"
          echo "Dry run parameter: ${{ inputs.dry_run || 'false' }}"
          echo "Detected workflow file change: ${{ contains(github.event.head_commit.modified, '.github/workflows/') }}"

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: true
      
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.0
      
      - name: Check backend resources
        id: check_backend
        run: |
          # Check if S3 bucket exists
          BUCKET="tf-state-aws-infra011"
          if ! aws s3api head-bucket --bucket "$BUCKET" 2>/dev/null; then
            echo "⚠️ S3 bucket for backend does not exist!"
            echo "Run bootstrap-infrastructure.yml workflow first"
            exit 1
          fi
          
          # Check if DynamoDB table exists
          TABLE="terraform-lock"
          if ! aws dynamodb describe-table --table-name "$TABLE" &>/dev/null; then
            echo "⚠️ DynamoDB table for locking does not exist!"
            echo "Run bootstrap-infrastructure.yml workflow first"
            exit 1
          fi
          
          echo "✅ Backend resources verified"
      
      - name: Terraform Init
        working-directory: terraform
        run: |
          # Enable debug mode if requested
          if [[ "${{ env.DEBUG_MODE }}" == "true" ]]; then
            export TF_LOG=DEBUG
          fi
          
          # Use workspace based on environment
          ENV=${{ inputs.environment || 'dev' }}
          terraform init
          
          # Create workspace if it doesn't exist
          terraform workspace select $ENV || terraform workspace new $ENV
      
      - name: Check for pending secrets deletion
        run: |
          ENV=${{ inputs.environment || 'dev' }}
          SECRET_ID="secure-app-infra/$ENV/app"
          
          if aws secretsmanager describe-secret --secret-id "$SECRET_ID" 2>/dev/null; then
            DELETE_DATE=$(aws secretsmanager describe-secret --secret-id "$SECRET_ID" --query "DeletedDate" --output text)
            
            if [[ "$DELETE_DATE" != "None" && -n "$DELETE_DATE" ]]; then
              echo "Secret is scheduled for deletion, forcing immediate deletion..."
              aws secretsmanager delete-secret --secret-id "$SECRET_ID" --force-delete-without-recovery
              echo "Waiting for deletion to complete..."
              sleep 10
            fi
          fi
      
      - name: Verify State Integrity and Import Resources
        working-directory: terraform
        run: |
          ENV=${{ inputs.environment || 'dev' }}
          
          # Create a safer import script with error handling
          cat > import_resources.sh << 'EOF'
          #!/bin/bash
          set -eo pipefail
          
          function import_resource() {
            local resource_address="$1"
            local resource_id="$2"
            
            # Check if resource already exists in state
            if terraform state list | grep -q "^$resource_address\$"; then
              echo "✅ Resource $resource_address already in state, skipping import"
              return 0
            fi
            
            echo "Attempting to import $resource_id into $resource_address"
            if terraform import $resource_address $resource_id 2>/dev/null; then
              echo "✅ Successfully imported $resource_id"
              return 0
            else
              echo "⚠️ Could not import $resource_id (it may not exist yet)"
              return 0  # Don't fail the build
            fi
          }
          
          # Import required existing resources
          ENV="${ENV}"
          import_resource "module.monitoring.aws_cloudwatch_log_group.app_logs" "/aws/ec2/secure-app-infra-$ENV-app" || true
          import_resource "module.secrets.aws_secretsmanager_secret.app_secrets" "secure-app-infra/$ENV/app" || true
          import_resource "module.secrets.aws_iam_role.ec2_secrets_role" "secure-app-infra-$ENV-ec2-secrets-role" || true
          
          # Get current account ID
          ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          import_resource "module.secrets.aws_iam_policy.secrets_access" "arn:aws:iam::$ACCOUNT_ID:policy/secure-app-infra-$ENV-secrets-access" || true
          import_resource "module.secrets.aws_iam_instance_profile.ec2_profile" "secure-app-infra-$ENV-ec2-profile" || true
          import_resource "module.asg.aws_launch_template.app" "secure-app-infra-$ENV-launch-template" || true
          
          exit 0
          EOF
          
          chmod +x import_resources.sh
          ./import_resources.sh
      
      - name: Determine AMI ID to use
        id: ami
        run: |
          if [[ "${{ needs.check_prerequisites.outputs.ami_rebuild_required }}" == "true" ]]; then
            # Use AMI from the rebuild job
            AMI_ID="${{ needs.trigger_ami_build.outputs.ami_id }}"
          else
            # Use AMI from the check job
            AMI_ID="${{ needs.check_prerequisites.outputs.ami_id }}"
          fi
          
          if [[ -z "$AMI_ID" ]]; then
            echo "⚠️ No AMI ID found, will use default from Terraform"
            echo "ami_id=" >> $GITHUB_OUTPUT
          else
            echo "Using AMI ID: $AMI_ID"
            echo "ami_id=$AMI_ID" >> $GITHUB_OUTPUT
          fi
      
      - name: Terraform Plan
        id: plan
        working-directory: terraform
        run: |
          # Enable debug mode if requested
          if [[ "${{ env.DEBUG_MODE }}" == "true" ]]; then
            export TF_LOG=DEBUG
          fi
          
          # Create variable file with AMI ID if available
          cat > terraform.tfvars << EOF
          environment = "${{ inputs.environment || 'dev' }}"
          EOF
          
          # Add AMI ID if available
          if [[ -n "${{ steps.ami.outputs.ami_id }}" ]]; then
            echo "app_ami_id = \"${{ steps.ami.outputs.ami_id }}\"" >> terraform.tfvars
          fi
          
          # Run plan with retries
          MAX_RETRIES=3
          for i in $(seq 1 $MAX_RETRIES); do
            echo "Planning attempt $i/$MAX_RETRIES"
            if terraform plan -out=tfplan; then
              echo "✅ Plan successful"
              # Save plan summary for artifacts
              terraform show -no-color tfplan > plan_summary.txt
              break
            else
              if [ $i -eq $MAX_RETRIES ]; then
                echo "❌ Failed to plan after $MAX_RETRIES attempts"
                exit 1
              else
                echo "Retrying in 10 seconds..."
                sleep 10
              fi
            fi
          done
      
      - name: Upload plan summary as artifact
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan-${{ inputs.environment || 'dev' }}
          path: terraform/plan_summary.txt
          retention-days: 14
      
      - name: Terraform Apply
        if: inputs.dry_run != true
        working-directory: terraform
        run: |
          # Enable debug mode if requested
          if [[ "${{ env.DEBUG_MODE }}" == "true" ]]; then
            export TF_LOG=DEBUG
          fi
          
          # Run apply with retry logic
          MAX_RETRIES=2
          for i in $(seq 1 $MAX_RETRIES); do
            echo "Apply attempt $i/$MAX_RETRIES"
            if terraform apply -auto-approve tfplan; then
              echo "✅ Apply successful"
              break
            else
              if [ $i -eq $MAX_RETRIES ]; then
                echo "❌ Failed to apply after $MAX_RETRIES attempts"
                exit 1
              else
                echo "Retrying in 30 seconds..."
                sleep 30
              fi
            fi
          done

      - name: Terraform Apply (Fallback)
        if: failure()
        working-directory: terraform
        run: |
          echo "Previous apply attempt failed. Trying direct apply..."
          if [[ -n "${{ steps.ami.outputs.ami_id }}" ]]; then
            terraform apply -auto-approve -var="app_ami_id=${{ steps.ami.outputs.ami_id }}"
          else
            terraform apply -auto-approve
          fi    
      
      - name: Get Terraform Outputs
        id: terraform_outputs
        working-directory: terraform
        run: |
          # Use jq to safely extract outputs
          set +e
          
          # Try to get bastion IP with fallback
          if BASTION_JSON=$(terraform output -json bastion_public_ip 2>/dev/null); then
            BASTION_IP=$(echo "$BASTION_JSON" | jq -r 'if type=="string" then . else empty end')
            echo "Extracted Bastion IP: $BASTION_IP"
          else
            echo "Warning: bastion_public_ip output not found or has wrong format"
            BASTION_IP=""
          fi
          
          # Try to get ALB DNS with fallback
          if ALB_JSON=$(terraform output -json alb_dns_name 2>/dev/null); then
            ALB_DNS=$(echo "$ALB_JSON" | jq -r 'if type=="string" then . else empty end')
            echo "Extracted ALB DNS: $ALB_DNS"
          else
            echo "Warning: alb_dns_name output not found or has wrong format"
            ALB_DNS=""
          fi
          
          echo "bastion_ip=$BASTION_IP" >> $GITHUB_OUTPUT
          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT
      
      - name: Wait for Infrastructure with Healthcheck
        if: inputs.dry_run != true
        run: |
          ALB_DNS="${{ steps.terraform_outputs.outputs.alb_dns }}"
          
          if [[ -z "$ALB_DNS" ]]; then
            echo "⚠️ ALB DNS not available, skipping infrastructure healthcheck"
            exit 0
          fi
          
          echo "Waiting for infrastructure to initialize..."
          TIMEOUT=300
          START_TIME=$(date +%s)
          
          # Poll until ALB is healthy or timeout
          while true; do
            CURRENT_TIME=$(date +%s)
            ELAPSED_TIME=$((CURRENT_TIME - START_TIME))
            
            if [ $ELAPSED_TIME -gt $TIMEOUT ]; then
              echo "⚠️ Timeout waiting for infrastructure to become ready, but continuing..."
              exit 0
            fi
            
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" -m 5 http://$ALB_DNS || echo "000")
            
            if [[ "$HTTP_STATUS" == "200" ]]; then
              echo "✅ ALB is responding with 200 OK, infrastructure is ready!"
              break
            else
              echo "⏳ Waiting for ALB to become healthy (status: $HTTP_STATUS), elapsed time: ${ELAPSED_TIME}s"
              sleep 15
            fi
          done

  deploy:
    needs: terraform-infra
    if: |
      inputs.dry_run != true && 
      (github.event_name != 'push' || !contains(github.event.head_commit.modified, '.github/workflows/'))
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment || 'dev' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: true
      
      - name: Verify bastion host availability
        id: check_bastion
        run: |
          BASTION_IP="${{ needs.terraform-infra.outputs.bastion_ip }}"
          
          if [[ -z "$BASTION_IP" ]]; then
            echo "⚠️ No bastion host IP provided"
            echo "has_bastion=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Check if the host is reachable
          if nc -zv -w 5 $BASTION_IP 22 2>/dev/null; then
            echo "✅ Bastion host is reachable"
            echo "has_bastion=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ Bastion host is not reachable, will skip SSH-based deployment"
            echo "has_bastion=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install Ansible and dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ansible==8.5.0 boto3 botocore netaddr
          ansible-galaxy collection install amazon.aws community.general
          
      - name: Check for SSH key
        id: check_ssh_key
        run: |
          if [ -n "${{ secrets.SSH_PRIVATE_KEY }}" ]; then
            echo "has_ssh_key=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No SSH key provided in secrets. Will use dynamic inventory without SSH."
            echo "has_ssh_key=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Configure SSH for bastion access
        if: steps.check_bastion.outputs.has_bastion == 'true' && steps.check_ssh_key.outputs.has_ssh_key == 'true'
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          
          # Create SSH config with better security settings
          cat > ~/.ssh/config << EOF
          Host bastion
            HostName ${{ needs.terraform-infra.outputs.bastion_ip }}
            User ec2-user
            IdentityFile ~/.ssh/id_rsa
            StrictHostKeyChecking accept-new
            UserKnownHostsFile ~/.ssh/known_hosts
            ServerAliveInterval 60
            ServerAliveCountMax 10
            
          # Use IP address pattern matching for private instances
          Host 10.0.*.*
            User ec2-user
            IdentityFile ~/.ssh/id_rsa
            ProxyCommand ssh -q -W %h:%p bastion
            StrictHostKeyChecking accept-new
            UserKnownHostsFile ~/.ssh/known_hosts
            ServerAliveInterval 60
          EOF
          
          chmod 600 ~/.ssh/config
          
          # Test connection to bastion with timeout and retry
          echo "Testing connection to bastion host..."
          for i in {1..5}; do
            if timeout 30s ssh -o ConnectTimeout=10 bastion 'echo Connection successful'; then
              break
            else
              echo "Connection attempt $i failed, retrying in 10 seconds..."
              sleep 10
              if [ $i -eq 5 ]; then
                echo "⚠️ Failed to connect after 5 attempts! Will continue with AWS APIs instead."
              fi
            fi
          done
      
      - name: Run Ansible playbooks
        if: steps.check_bastion.outputs.has_bastion == 'true' && steps.check_ssh_key.outputs.has_ssh_key == 'true'
        run: |
          cd ansible
          
          # First verify ansible directory and essential files exist
          if [ ! -d "inventory" ]; then
            mkdir -p inventory
            echo "Created missing inventory directory"
          fi
          
          if [ ! -d "playbooks" ]; then
            mkdir -p playbooks
            echo "Created missing playbooks directory"
            
            # Create basic deploy playbook if missing
            cat > playbooks/deploy.yml << 'EOF'
            ---
            - name: Deploy application
              hosts: tag_Project_secure_app_infra
              gather_facts: yes
              become: yes
              tasks:
                - name: Ensure nginx is running
                  systemd:
                    name: nginx
                    state: started
                    enabled: yes
                  
                - name: Check application content
                  command: ls -la /usr/share/nginx/html/
                  register: app_content
                  changed_when: false
                  
                - name: Show application content
                  debug:
                    var: app_content.stdout_lines
            EOF
            
            # Create basic verify playbook if missing
            cat > playbooks/verify.yml << 'EOF'
            ---
            - name: Verify application deployment
              hosts: tag_Project_secure_app_infra
              gather_facts: yes
              become: yes
              tasks:
                - name: Check nginx status
                  systemd:
                    name: nginx
                    state: started
                  register: nginx_status
                  
                - name: Verify nginx is running
                  debug:
                    msg: "Nginx status: {{ nginx_status.status }}"
            EOF
          fi
          
          # Export environment variables for dynamic inventory
          export AWS_REGION=${{ env.AWS_REGION }}
          export ENVIRONMENT=${{ inputs.environment || 'dev' }}
          
          # Add dynamic inventory config
          cat > inventory/aws_ec2.yml << EOF
          ---
          plugin: aws_ec2
          regions:
            - ${{ env.AWS_REGION }}
          keyed_groups:
            - prefix: tag
              key: tags
            - key: tags.Project
              prefix: project
            - key: tags.Environment
              prefix: env
            - key: tags.Name
              separator: ""
          filters:
            tag:ManagedBy: Terraform
            tag:Environment: $ENVIRONMENT
            instance-state-name: running
          compose:
            ansible_host: private_ip_address
          hostnames:
            - tag:Name
            - private-dns-name
            - private-ip-address
          EOF
          
          # Test connectivity to instances with retry
          echo "Testing Ansible inventory..."
          if ! ansible-inventory -i inventory/aws_ec2.yml --list; then
            echo "⚠️ Inventory issue detected. Waiting 30 seconds and retrying..."
            sleep 30
            ansible-inventory -i inventory/aws_ec2.yml --list
          fi
          
          # Test connectivity to instances
          echo "Testing connectivity to instances..."
          if ANSIBLE_HOST_KEY_CHECKING=False ansible all -i inventory/aws_ec2.yml -m ping; then
            echo "✅ Successfully connected to all instances"
            
            # Run application deployment
            ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory/aws_ec2.yml playbooks/deploy.yml
            
            # Run verification playbook
            ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory/aws_ec2.yml playbooks/verify.yml
          else
            echo "⚠️ Could not connect to instances via SSH, skipping Ansible playbooks"
          fi
      
      - name: Non-SSH deployment actions
        if: steps.check_bastion.outputs.has_bastion != 'true' || steps.check_ssh_key.outputs.has_ssh_key != 'true'
        run: |
          echo "Using AWS API to verify deployment (no SSH access available)"
          
          # Get ASG information
          ENV="${{ inputs.environment || 'dev' }}"
          ASG_NAME=$(aws autoscaling describe-auto-scaling-groups \
            --filters "Name=tag:Environment,Values=$ENV" "Name=tag:Project,Values=secure-app-infra" \
            --query "AutoScalingGroups[0].AutoScalingGroupName" --output text)
            
          if [[ "$ASG_NAME" == "None" || -z "$ASG_NAME" ]]; then
            echo "⚠️ No ASG found for verification"
          else
            echo "✅ Found ASG: $ASG_NAME"
            ASG_INSTANCES=$(aws autoscaling describe-auto-scaling-groups \
              --auto-scaling-group-name "$ASG_NAME" \
              --query "AutoScalingGroups[0].Instances[*].InstanceId" --output text)
              
            echo "ASG Instances: $ASG_INSTANCES"
            
            # Check if instances are healthy
            echo "Checking instance health status..."
            for INSTANCE_ID in $ASG_INSTANCES; do
              STATUS=$(aws ec2 describe-instance-status \
                --instance-ids "$INSTANCE_ID" \
                --query "InstanceStatuses[0].InstanceStatus.Status" --output text)
                
              echo "Instance $INSTANCE_ID status: $STATUS"
            done
          fi

  verify:
    needs: [terraform-infra, deploy]
    if: |
      inputs.dry_run != true && 
      (github.event_name != 'push' || !contains(github.event.head_commit.modified, '.github/workflows/'))
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment || 'dev' }}
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Verify deployment
        run: |
          ALB_DNS="${{ needs.terraform-infra.outputs.alb_dns }}"
          
          if [[ -z "$ALB_DNS" ]]; then
            echo "⚠️ No ALB DNS provided, skipping HTTP verification"
            # Try to find ALB DNS from AWS instead
            ENV="${{ inputs.environment || 'dev' }}"
            ALB_DNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, 'secure-app-infra-$ENV')].DNSName" \
              --output text)
              
            if [[ -z "$ALB_DNS" || "$ALB_DNS" == "None" ]]; then
              echo "⚠️ Could not find ALB from AWS API either, verification inconclusive"
              exit 0
            else
              echo "Found ALB DNS from AWS API: $ALB_DNS"
            fi
          fi
          
          # Comprehensive health check with retries
          MAX_RETRIES=10
          RETRY=0
          
          while [ $RETRY -lt $MAX_RETRIES ]; do
            echo "Test attempt $((RETRY+1))/$MAX_RETRIES - checking website at http://${ALB_DNS}..."
            
            HTTP_STATUS=$(curl -s -o response.html -w "%{http_code}" -m 10 http://${ALB_DNS}/)
            
            if [[ "$HTTP_STATUS" == "200" ]]; then
              echo "✅ Website is accessible! HTTP status: $HTTP_STATUS"
              
              # Verify content exists (but be lenient about exact content)
              if [[ -s response.html ]]; then
                echo "✅ Content verification successful"
                cat response.html | head -10
                
                # Check page load time
                LOAD_TIME=$(curl -s -w "%{time_total}\n" -o /dev/null -m 10 http://${ALB_DNS}/)
                echo "Page load time: ${LOAD_TIME}s"
                
                echo "Deployment verified successfully at $(date -u)"
                
                # Extract version info if available
                if grep -q "version" response.html; then
                  VERSION=$(grep -o "version[^<]*" response.html || echo "Version not found")
                  echo "Deployed application version: $VERSION"
                fi
                
                exit 0
              else
                echo "⚠️ Content verification failed - no content returned"
              fi
            else
              echo "⚠️ Website not accessible. Status code: $HTTP_STATUS"
              RETRY=$((RETRY+1))
              
              if [ $RETRY -eq $MAX_RETRIES ]; then
                echo "⚠️ Website verification did not complete successfully after $MAX_RETRIES attempts"
                echo "This might be normal if the infrastructure is still initializing"
                echo "Check the application manually at http://${ALB_DNS}"
                exit 0
              else
                echo "Retrying in 30 seconds..."
                sleep 30
              fi
            fi
          done

  notify:
    needs: [terraform-infra, deploy, verify]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Generate deployment summary
        run: |
          ENVIRONMENT="${{ inputs.environment || 'dev' }}"
          STATUS="success"
          
          if [[ "${{ needs.terraform-infra.result }}" != "success" || 
                ("${{ inputs.dry_run }}" != "true" && 
                 ("${{ needs.deploy.result }}" != "success" || 
                  "${{ needs.verify.result }}" != "success")) ]]; then
            STATUS="failure"
          fi
          
          if [[ "${{ inputs.dry_run }}" == "true" ]]; then
            echo "Deployment Type: Dry Run (Plan Only)" > deployment_summary.txt
          else
            echo "Deployment Type: Full Deployment" > deployment_summary.txt
          fi
          
          echo "Environment: $ENVIRONMENT" >> deployment_summary.txt
          echo "Status: $STATUS" >> deployment_summary.txt
          echo "Deployed by: ${{ github.actor }}" >> deployment_summary.txt
          echo "Timestamp: $(date -u)" >> deployment_summary.txt
          
          if [[ -n "${{ needs.terraform-infra.outputs.alb_dns }}" ]]; then
            echo "Application URL: http://${{ needs.terraform-infra.outputs.alb_dns }}" >> deployment_summary.txt
          fi
          
          echo "Workflow Run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> deployment_summary.txt
          
          cat deployment_summary.txt
      
      - name: Upload deployment summary
        uses: actions/upload-artifact@v4
        with:
          name: deployment-summary-${{ inputs.environment || 'dev' }}-${{ github.run_id }}
          path: deployment_summary.txt
          retention-days: 30
